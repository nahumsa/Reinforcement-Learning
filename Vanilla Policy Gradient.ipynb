{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla Policy Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNL4T6pKt/eqIB+bo2TaFN5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nahumsa/Reinforcement-Learning/blob/master/Vanilla%20Policy%20Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxVAY_tC2b7P",
        "colab_type": "text"
      },
      "source": [
        "# Vanilla Policy Gradient\n",
        "\n",
        "In this notebook I will implement on Tensoflow 2 an implementation of the [Vanilla Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/vpg.html) as it is explained on the [Spinning Up](https://spinningup.openai.com/en/latest/index.html) by Open AI, which is an incredible resource for Reinforcement Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeFKJkUG5QTT",
        "colab_type": "text"
      },
      "source": [
        "# 0) Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm928IOw5QwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import base64, io, time, gym\n",
        "import IPython, functools\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhIsOFNp2k-n",
        "colab_type": "text"
      },
      "source": [
        "# 1) Policy Gradient\n",
        "\n",
        "Firstly, we need to define how to efficiently calculate the Policy Gradient, this is done by updating the expected return: \n",
        "$$\n",
        "J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\big[ R(\\tau) \\big]\n",
        "$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZ6ZX0R4_gF",
        "colab_type": "text"
      },
      "source": [
        "## 1.1) Return Function\n",
        "\n",
        "There are severous return functions. In this section I will show some implmentations of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e7LyDg5EX_m",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.1) Finite-horizon undiscounted return\n",
        "This return is defined by: \n",
        "\n",
        "$$\n",
        "R(\\tau) = \\sum_{t=0}^{\\infty} r_t\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzHSCrKf4-V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalized(x): \n",
        "  x -= np.mean(x)\n",
        "  x /= np.std(x)\n",
        "  return x.astype(np.float32)\n",
        "\n",
        "def undiscount_rewards(rewards):\n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  # Do a backward update on rewards\n",
        "  for t in reversed(range(0,len(rewards))):\n",
        "    R = R + rewards[t]\n",
        "    undiscounted_rewards[t] = R\n",
        "  \n",
        "  return normalized(undiscounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVOBVBfyEZk8",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.2) Finite-horizon discounted return\n",
        "This return is defined by: \n",
        "\n",
        "$$\n",
        "R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cccuhiNCEelt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_rewards(rewards, gamma=0.99):\n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  # Do a backward update on rewards\n",
        "  for t in reversed(range(0,len(rewards))):\n",
        "    R = R*gamma + rewards[t]\n",
        "    discounted_rewards[t] = R\n",
        "  \n",
        "  return normalized(discounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4_xFuMk6C1L",
        "colab_type": "text"
      },
      "source": [
        "# 1.2) Optimizing the Policy\n",
        "\n",
        "We would like to optimize the policy by gradient ascent:\n",
        "\n",
        "$$\n",
        "\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_\\theta J(\\pi_\\theta) |_{\\theta_{k}}\n",
        "$$\n",
        "\n",
        "We define the policy gradient as the gradient of the policy performance ($\\nabla_\\theta J(\\pi_\\theta)$) and algorithms that optimize the policy are called **policy gradient algorithms**.\n",
        "\n",
        "We need a expression that makes it easier to calculade the policy gradient, this can be done by the following:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\pi_\\theta) = \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\big[ R(\\tau) \\big] = \\nabla_\\theta \\int_{\\tau} P(\\tau | \\theta) R(\\tau) =  \\int_{\\tau} \\big( \\nabla_\\theta P(\\tau | \\theta) \\big) R(\\tau)\n",
        "$$\n",
        "\n",
        "For calculating the term in parenthesis we use the log-derivative trick, which combines the derivative of the log of a function and the chain rule:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta  \\mathrm{log}P(\\tau | \\theta) = \\frac{\\nabla_\\theta P(\\tau | \\theta)}{P(\\tau | \\theta)} \\Rightarrow \\nabla_\\theta  P(\\tau | \\theta) = P(\\tau | \\theta)\\nabla_\\theta  \\mathrm{log}P(\\tau | \\theta)\n",
        "$$\n",
        "\n",
        "Then we have:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\pi_\\theta) = \\int_{\\tau} \\big( P(\\tau | \\theta)\\nabla_\\theta  \\mathrm{log}P(\\tau | \\theta) \\big) R(\\tau) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\bigg[ \\nabla_\\theta  \\mathrm{log}P(\\tau | \\theta) R(\\tau) \\bigg]\n",
        "$$\n",
        "\n",
        "To get a final expression, we need to see the probability of a trajectory given that actions come fom $\\pi_\\theta$ is:\n",
        "\n",
        "$$\n",
        "P(\\tau|\\theta) = \\rho(s_0) \\prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \\pi_\\theta (a_t|s_t)\n",
        "$$\n",
        "\n",
        "Then the log of that probability is:\n",
        "\n",
        "$$\n",
        "\\mathrm{log} P(\\tau|\\theta) = \\rho(s_0) +  \\sum_{t=0}^{T} \\bigg( \\mathrm{log}P(s_{t+1}|s_t, a_t) + \\mathrm{log} \\pi_\\theta (a_t|s_t) \\bigg)\n",
        "$$\n",
        "\n",
        "Then we have the policy gradient, noting that the only term that doesn't have gradient 0 is the term with $\\pi_\\theta$:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\bigg[ \\sum_{t=0}^T \\nabla_\\theta \\mathrm{log} \\pi_\\theta(a_t|s_t)  R(\\tau) \\bigg]\n",
        "$$\n",
        "\n",
        "Since this is an expectation value we can estimate it by sampling the mean of collected trajectories, D:\n",
        "\n",
        "$$ \n",
        "\\hat{g} = \\frac{1}{|D|} \\sum_{\\tau \\in D} \\sum_{t=0}^T \\nabla_\\theta \\mathrm{log} \\pi_\\theta(a_t|s_t)  R(\\tau)\n",
        "$$\n",
        "\n",
        "Where $|D|$ is the number of trajectories in D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zHZXv6fDN8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory:\n",
        "  \"\"\" Memory used in training in order to keep each \n",
        "  Observation, Actions and Rewards.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.clear()\n",
        "  \n",
        "  def clear(self):\n",
        "    self.observations = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "\n",
        "  def add_to_memory(self, new_observation, new_action, new_reward):\n",
        "    self.observations.append(new_observation)\n",
        "    self.actions.append(new_action)\n",
        "    self.rewards.append(new_reward)\n",
        "\n",
        "memory = Memory()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TQL8TFj7s6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_action(model, observation):\n",
        "  \"\"\"Choosing an action from a model.\n",
        "  \"\"\" \n",
        "  observation = np.expand_dims(observation, axis=0)\n",
        "  logits = model.predict(observation)\n",
        "  \n",
        "  #Policy\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()\n",
        "  \n",
        "  #Chosing an action according to the policy\n",
        "  action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0]  \n",
        "  return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5mnvx-CCPkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(logits, actions, rewards):\n",
        "  \"\"\"Loss function for the Vanilla Policy Gradient\n",
        "  \"\"\"\n",
        "  neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=actions)\n",
        "  loss = tf.reduce_mean(rewards*neg_logprob)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR9kvBGzCekQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  \"\"\"Gradient ascend step for the model\n",
        "  \"\"\"\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    #Forwardprop\n",
        "    logits = model(observations)\n",
        "    loss = compute_loss(logits,actions,discounted_rewards)\n",
        "  \n",
        "  #Backprop\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads,model.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvl92DGdCrEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5000\n",
        "epochs = range(epochs)\n",
        "\n",
        "def training_loop()\n",
        "   \n",
        "  observation = env.reset()\n",
        "  memory.clear()\n",
        "\n",
        "  while True:\n",
        "    action = choose_action(car_model, observation)\n",
        "    next_observation, reward, done, info = env.step(action)\n",
        "\n",
        "    memory.add_to_memory(observation, action, reward)\n",
        "\n",
        "    if done:\n",
        "      total_reward = sum(memory.rewards)\n",
        "      smoothed_reward.append(total_reward)\n",
        "\n",
        "      train_step(car_model, optmizer,\n",
        "                observations= np.vstack(memory.observations),\n",
        "                actions=np.array(memory.actions),\n",
        "                discounted_rewards= discount_rewards(memory.rewards)\n",
        "                )\n",
        "      memory.clear()\n",
        "      break\n",
        "    \n",
        "    observation = next_observation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K_Th0hfHR1J",
        "colab_type": "text"
      },
      "source": [
        "# 2) Resources\n",
        "\n",
        "[Introduction to Policy Optimization](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\n",
        "\n",
        "[Vanilla Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/vpg.html)\n",
        "\n",
        "Schulman et al. - [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n",
        "\n",
        "Sutton et al - [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)"
      ]
    }
  ]
}